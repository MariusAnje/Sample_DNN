{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import Sample_AlexNet as Sample_AlexNet\n",
    "from SampleNN import *\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline\n",
    "#device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "traindir = '/home/yanzy/data/train'\n",
    "valdir = '/home/yanzy/data/val'\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.ImageFolder(valdir, transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "\n",
    "        ])),\n",
    "        batch_size=1, shuffle=False,\n",
    "        num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(Sample = 1, N = 16, m = 8):\n",
    "    TMP = protectStateDict(sample_net)\n",
    "    sampleStateDict(sample_net, N, m)\n",
    "    sample_net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for data in tqdm_notebook(val_loader, desc = \"m = %d\"%(m), leave = False):\n",
    "            if i == 82:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                images = mSample(N=N,m=m)(images)\n",
    "                ttt = torch.zeros(1,3,227,227)\n",
    "                ttt[:,:,2:226,2:226] = images\n",
    "                img = ttt.to(device)\n",
    "                outputs = sample_net(img)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                #print outputs*pow(2,m)\n",
    "                break\n",
    "            i = i+1\n",
    "\n",
    "    print('When m = %d, Accuracy of the network on the %d test images: %.3f %%' % (m, total,\n",
    "        100.0 * correct / total))\n",
    "    \n",
    "    sample_net.load_state_dict(TMP)\n",
    "    del TMP\n",
    "    return 100.0 * correct / total, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_float(Sample = 1, N = 16, m = 8):\n",
    "    float_net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm_notebook(val_loader, desc = \"m = %d\"%(m), leave = False):\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = float_net(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                #print correct, total\n",
    "                break\n",
    "\n",
    "    print('When m = %d, Accuracy of the network on the %d test images: %.3f %%' % (m, total,\n",
    "        100.0 * correct / total))\n",
    "    \n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Clamp version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_net = torchvision.models.alexnet(pretrained=True).to(device)\n",
    "gt  = float_net.state_dict()\n",
    "b = gt.keys()\n",
    "N = 16\n",
    "m = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 11, 11])\n",
      "torch.Size([64])\n",
      "torch.Size([192, 64, 5, 5])\n",
      "torch.Size([192])\n",
      "torch.Size([384, 192, 3, 3])\n",
      "torch.Size([384])\n",
      "torch.Size([256, 384, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([4096, 9216])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([1000, 4096])\n",
      "torch.Size([1000])\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "sample_net = Sample_AlexNet.alexnet().to(device)\n",
    "lol = sample_net.state_dict()\n",
    "a = lol.keys()\n",
    "for i in range(len(a)):\n",
    "    lol[a[i]] = gt[b[i]]\n",
    "    print (lol[a[i]].shape)\n",
    "sample_net.load_state_dict(lol)\n",
    "sampleStateDict(sample_net, N, m)\n",
    "sample_net.eval()\n",
    "print (len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f7f6391c284d8cb28cdec5ca581055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SEJveChjaGlsZHJlbj0oSW50UHJvZ3Jlc3ModmFsdWU9MCwgZGVzY3JpcHRpb249dSdtID0gOCcsIG1heD01MDAwMCwgc3R5bGU9UHJvZ3Jlc3NTdHlsZShkZXNjcmlwdGlvbl93aWR0aD11J2nigKY=\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When m = 8, Accuracy of the network on the 1 test images: 100.000 %\n",
      "sdfsdf\n"
     ]
    }
   ],
   "source": [
    "sample_net.eval()\n",
    "res, images = val(1,N,8)\n",
    "print 'sdfsdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad14e17f24743fab917e44ba712b18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SEJveChjaGlsZHJlbj0oSW50UHJvZ3Jlc3ModmFsdWU9MCwgZGVzY3JpcHRpb249dSdtID0gOCcsIG1heD01MDAwMCwgc3R5bGU9UHJvZ3Jlc3NTdHlsZShkZXNjcmlwdGlvbl93aWR0aD11J2nigKY=\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "When m = 8, Accuracy of the network on the 1 test images: 100.000 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2347998892484988a6e99d7df6321af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SEJveChjaGlsZHJlbj0oSW50UHJvZ3Jlc3ModmFsdWU9MCwgZGVzY3JpcHRpb249dSdtID0gOCcsIG1heD01MDAwMCwgc3R5bGU9UHJvZ3Jlc3NTdHlsZShkZXNjcmlwdGlvbl93aWR0aD11J2nigKY=\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "happysfsd = 0\n",
    "with torch.no_grad():\n",
    "    for data in tqdm_notebook(val_loader, desc = \"m = %d\"%(m), leave = False):\n",
    "            images, labels = data\n",
    "            if labels == 23:\n",
    "                happysfsd = happysfsd + 1\n",
    "            if happysfsd == 13:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([14.3750], device='cuda:0'), tensor([138], device='cuda:0'))\n",
      "tensor([23])\n"
     ]
    }
   ],
   "source": [
    "print torch.max(sample_net(mSample(N=16,m=8)(images).to(device)).data, 1)\n",
    "print labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = (images.to(torch.float32)/pow(2,-8)).round().clamp(-pow(2,16),pow(2,16)-1).to(torch.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 11, 11])\n"
     ]
    }
   ],
   "source": [
    "a = torch.load('./Weights/features.0.weight.pt')\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 16\n",
    "m = 8\n",
    "delt = pow(2,-m)\n",
    "Q = pow(2, N-1) - 1\n",
    "ImageSize = 224\n",
    "img = torchvision.transforms.ToTensor()(plt.imread('timg.jpg'))\n",
    "img = nn.functional.interpolate(img.view(1,3,650,1200),size=(ImageSize,ImageSize),mode='bilinear')\n",
    "img = (img/delt).to(torch.int16)\n",
    "img = img.to(torch.float) * delt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_net.eval()\n",
    "a = open('test.csv', 'w+')\n",
    "for i in list((mSample(N,m)(sample_net(img))/delt).view(-1).detach().numpy()):\n",
    "    a.write('%d,'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(803)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_net(img).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(803)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_net.eval()\n",
    "float_net(img).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7647, 0.7569, 0.7490,  ..., 0.6549, 0.6627, 0.6627],\n",
       "         [0.7686, 0.7608, 0.7647,  ..., 0.6471, 0.6667, 0.6667],\n",
       "         [0.7686, 0.7569, 0.7725,  ..., 0.6471, 0.6353, 0.6431],\n",
       "         ...,\n",
       "         [0.4078, 0.3804, 0.4157,  ..., 0.2706, 0.5373, 0.5882],\n",
       "         [0.3922, 0.3843, 0.3843,  ..., 0.2157, 0.4471, 0.5843],\n",
       "         [0.5059, 0.5176, 0.5137,  ..., 0.2078, 0.3529, 0.5255]],\n",
       "\n",
       "        [[0.7294, 0.7255, 0.7176,  ..., 0.6549, 0.6627, 0.6627],\n",
       "         [0.7333, 0.7255, 0.7294,  ..., 0.6392, 0.6588, 0.6588],\n",
       "         [0.7412, 0.7294, 0.7451,  ..., 0.6392, 0.6275, 0.6353],\n",
       "         ...,\n",
       "         [0.2902, 0.2588, 0.2941,  ..., 0.2353, 0.4431, 0.5412],\n",
       "         [0.2706, 0.2627, 0.2627,  ..., 0.2000, 0.3804, 0.5255],\n",
       "         [0.4314, 0.4431, 0.4392,  ..., 0.1922, 0.2824, 0.4706]],\n",
       "\n",
       "        [[0.6941, 0.6824, 0.6745,  ..., 0.6549, 0.6627, 0.6627],\n",
       "         [0.6980, 0.6902, 0.6941,  ..., 0.6510, 0.6706, 0.6706],\n",
       "         [0.7020, 0.6902, 0.7059,  ..., 0.6510, 0.6392, 0.6471],\n",
       "         ...,\n",
       "         [0.2980, 0.2784, 0.3137,  ..., 0.2157, 0.3490, 0.4471],\n",
       "         [0.2902, 0.2824, 0.2824,  ..., 0.2039, 0.3529, 0.4980],\n",
       "         [0.4392, 0.4510, 0.4471,  ..., 0.1961, 0.2745, 0.4588]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.transforms.ToTensor()(plt.imread('timg.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load('conv1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-277\n",
      "-383\n",
      "-427\n",
      "-412\n",
      "-386\n",
      "-464\n",
      "-483\n",
      "-449\n",
      "-475\n",
      "-446\n",
      "-417\n",
      "-425\n",
      "-391\n",
      "-384\n",
      "-370\n",
      "-426\n",
      "-364\n",
      "-401\n",
      "-450\n",
      "-518\n",
      "-554\n",
      "-617\n",
      "-1062\n",
      "178\n",
      "61\n",
      "-289\n",
      "-298\n",
      "-295\n",
      "-304\n",
      "-254\n",
      "-329\n",
      "-291\n",
      "-243\n",
      "-214\n",
      "-302\n",
      "-307\n",
      "-339\n",
      "-348\n",
      "-384\n",
      "-365\n",
      "-289\n",
      "-245\n",
      "-266\n",
      "-347\n",
      "-371\n",
      "-323\n",
      "-353\n",
      "-148\n",
      "43\n",
      "-158\n",
      "-234\n",
      "-251\n",
      "-204\n",
      "-297\n",
      "-319\n"
     ]
    }
   ],
   "source": [
    "for i in range(55):\n",
    "    for j in range(55):\n",
    "        print int(a[0,0,i,j])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.,   0., 141., 154., 146., 154., 154., 168., 168., 163., 154., 150.,\n",
      "        150., 159., 168., 168., 168., 163., 168., 176., 176., 163., 172., 168.,\n",
      "        154., 154., 163., 168., 163., 168., 168., 168., 168., 159., 154., 159.,\n",
      "        150., 154., 159., 159., 154., 154., 163., 163., 154., 154., 163., 168.,\n",
      "        168., 172., 185., 172., 172., 172., 181., 181., 176., 163., 168., 172.,\n",
      "        168., 163., 176., 168., 181., 172., 163., 168., 163., 172., 176., 168.,\n",
      "        168., 163., 185., 185., 185., 189., 181., 181., 189., 185., 168., 172.,\n",
      "        181., 198., 194., 181., 176., 181., 198., 207., 207., 203., 185., 216.,\n",
      "        189., 172., 172., 172., 172., 168., 163., 172., 172., 159., 159., 163.,\n",
      "        159., 154., 150., 150., 159., 168., 163., 163., 163., 163., 150., 146.,\n",
      "        124., 128., 119., 106., 111., 115., 106., 102., 111., 111., 128., 132.,\n",
      "        137., 132., 150., 154., 163., 154., 163., 159., 159., 159., 163., 163.,\n",
      "        168., 176., 172., 172., 172., 168., 181., 176., 168., 172., 176., 168.,\n",
      "        150., 159., 154., 159., 168., 172., 176., 189., 185., 185., 181., 176.,\n",
      "        168., 154., 146., 137., 141., 141., 141., 137., 150., 146., 159., 163.,\n",
      "        176., 181., 185., 185., 189., 194., 194., 185., 189., 194., 194., 194.,\n",
      "        220., 268., 251., 203., 194., 194., 203., 216., 216., 207., 211., 216.,\n",
      "        203., 198., 207., 211., 211., 216., 216., 211., 211., 216., 216., 207.,\n",
      "        207., 203., 207., 203., 207., 207., 216., 216., 211., 216.,   0.])\n"
     ]
    }
   ],
   "source": [
    "print (images*256)[0,0,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features.0.weight',\n",
       " 'features.0.bias',\n",
       " 'features.5.weight',\n",
       " 'features.5.bias',\n",
       " 'features.10.weight',\n",
       " 'features.10.bias',\n",
       " 'features.14.weight',\n",
       " 'features.14.bias',\n",
       " 'features.18.weight',\n",
       " 'features.18.bias',\n",
       " 'classifier.2.weight',\n",
       " 'classifier.2.bias',\n",
       " 'classifier.7.weight',\n",
       " 'classifier.7.bias',\n",
       " 'classifier.11.weight',\n",
       " 'classifier.11.bias']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = sample_net.state_dict()\n",
    "st.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-248., -719.,   -9.,  -20.,  -30.,    6.,  -19., -363.,  421.,  -25.,\n",
       "          -4.,  -33.,  -17.,   -9.,  -19., -332.,  -13.,    3.,  -26., -304.,\n",
       "         -35.,  -13.,  -20.,  -10.,  -25.,  -18., -496.,  -22.,  -36.,  -50.,\n",
       "         -33., -514.,  -12.,  -15.,   -9.,  -98., -712.,  169.,  -42., -545.,\n",
       "          14.,   -7.,  -44.,  -15., -108., -496., -310.,    4.,  -28.,   -6.,\n",
       "         -38., -474.,  -24.,   -5.,  -18.,  -15.,  -16.,  -18., -325.,  -30.,\n",
       "         -11.,  -83.,   13.,   -4.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mSample(16,8)(st['features.0.bias'])*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -298,  -404,  -449,  ...,  -241,  -337,  -356],\n",
       "          [ -255,  -308,  -332,  ...,  -340,  -358,  -352],\n",
       "          [ -219,  -302,  -253,  ...,  -412,  -397,  -328],\n",
       "          ...,\n",
       "          [ -342,  -197,  -267,  ...,  -216,  -226,  -292],\n",
       "          [ -339,  -192,  -271,  ...,  -258,  -291,  -202],\n",
       "          [ -358,  -252,  -269,  ...,  -311,  -352,  -214]],\n",
       "\n",
       "         [[-1445, -1503, -1503,  ..., -1953, -1939, -2420],\n",
       "          [-1224, -1236, -1254,  ..., -1732, -1696, -2231],\n",
       "          [-1209, -1199, -1224,  ..., -1713, -1721, -2227],\n",
       "          ...,\n",
       "          [ -430,  -477,  -479,  ...,   229,   268,   868],\n",
       "          [ -406,  -452,  -448,  ...,   318,   286,   923],\n",
       "          [ -355,  -394,  -396,  ...,   496,   433,  1016]],\n",
       "\n",
       "         [[  -52,  -179,   -75,  ...,  -119,   -40,   -27],\n",
       "          [  -70,   -50,   -47,  ...,   -77,  -120,   -90],\n",
       "          [  -37,   -87,   -91,  ...,    22,  -134,  -152],\n",
       "          ...,\n",
       "          [   65,   -98,     9,  ...,   162,    31,   -56],\n",
       "          [   31,  -125,    21,  ...,    -9,    99,   -38],\n",
       "          [   21,   -95,   -73,  ...,  -139,    81,   -48]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ -152,  -118,  -111,  ...,  -106,  -125,  -160],\n",
       "          [ -144,  -119,  -128,  ...,   -84,  -100,  -124],\n",
       "          [ -121,   -92,   -96,  ...,  -135,  -127,  -137],\n",
       "          ...,\n",
       "          [  -31,   -52,   -80,  ...,   -84,   -85,   -51],\n",
       "          [  -20,   -86,   -63,  ...,  -132,  -134,   -48],\n",
       "          [ -584,  -716,  -746,  ..., -1562, -1620, -1518]],\n",
       "\n",
       "         [[  207,   219,   221,  ...,   644,   660,   664],\n",
       "          [ -101,  -149,  -149,  ...,   -31,    -2,     8],\n",
       "          [   15,    -7,    -2,  ...,    -9,   -20,    -7],\n",
       "          ...,\n",
       "          [  -44,   -78,   -65,  ...,  -137,   -89,   -68],\n",
       "          [  -53,   -65,   -84,  ...,  -146,  -142,   -88],\n",
       "          [   40,    35,    36,  ...,   290,   257,   283]],\n",
       "\n",
       "         [[   23,    -5,   -13,  ...,   -35,    35,   -55],\n",
       "          [  -43,    51,    21,  ...,   -41,   -22,   -21],\n",
       "          [   12,   -11,     0,  ...,     0,    -4,   -48],\n",
       "          ...,\n",
       "          [  -16,     3,    40,  ...,   -36,    35,     0],\n",
       "          [  -34,   -15,   -21,  ...,    -6,    -6,     6],\n",
       "          [   27,    -6,   -13,  ...,    13,    69,  -117]]]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=0)\n",
    "stconv = conv2d.state_dict()\n",
    "stconv['weight'] = mSample(16,8)(st['features.0.weight'])\n",
    "stconv['bias'] = mSample(16,8)(st['features.0.bias'])\n",
    "conv2d.load_state_dict(stconv)\n",
    "(mSample(16,8)(conv2d(images))*256).to(torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-28, -47, -32,  ..., -53, -66, -28],\n",
       "          [ -5, -38, -48,  ..., -22, -53, -25],\n",
       "          [ -8, -38, -28,  ..., -52, -50, -42],\n",
       "          ...,\n",
       "          [ 12,  20,  26,  ...,  66, 129,  58],\n",
       "          [ 14,  30,  30,  ...,  82, 144,  55],\n",
       "          [-40,  -4, -40,  ...,  -1, -14, -50]]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=11, stride=4, padding=0,bias=False)\n",
    "stconv = conv2d.state_dict()\n",
    "#st['features.0.weight'][0,2,:,:] 0- inpur channel, 2 output channel\n",
    "stconv['weight'] = mSample(16,8)(st['features.0.weight'][0,2,:,:].view(1,1,11,11))\n",
    "conv2d.load_state_dict(stconv)\n",
    "#conv2d(images[0,0,:,:] 00 - input 0, 01 input 1, 02 input 2\n",
    "(mSample(16,8)(conv2d(images[0,0,:,:].view(1,1,227,227)))*256).to(torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 0\n",
      "24 0\n",
      "24 0\n",
      "27 0\n",
      "26 0\n",
      "17 0\n",
      "13 0\n",
      "13 0\n",
      "14 0\n",
      "6 0\n",
      "13 0\n",
      "19 0\n",
      "10 0\n",
      "14 0\n",
      "19 0\n",
      "19 0\n",
      "19 0\n",
      "13 0\n",
      "7 0\n",
      "7 0\n",
      "-3 0\n",
      "1 0\n",
      "19 0\n",
      "10 0\n",
      "14 142\n",
      "14 155\n",
      "13 146\n",
      "13 155\n",
      "12 155\n",
      "6 168\n",
      "11 168\n",
      "3 164\n",
      "3 155\n",
      "18 0\n",
      "13 0\n",
      "16 150\n",
      "16 146\n",
      "15 146\n",
      "10 159\n",
      "12 150\n",
      "10 159\n",
      "12 164\n",
      "0 168\n",
      "1 159\n",
      "22 0\n",
      "19 0\n",
      "18 142\n",
      "21 137\n",
      "24 142\n",
      "17 146\n",
      "9 155\n",
      "5 159\n",
      "6 155\n",
      "-3 168\n",
      "-9 168\n",
      "25 0\n",
      "25 0\n",
      "26 133\n",
      "28 142\n",
      "19 142\n",
      "9 142\n",
      "-2 146\n",
      "-11 150\n",
      "-10 150\n",
      "-15 164\n",
      "-14 164\n",
      "29 0\n",
      "30 0\n",
      "27 133\n",
      "23 133\n",
      "1 142\n",
      "-23 146\n",
      "-29 150\n",
      "-36 146\n",
      "-32 142\n",
      "-22 159\n",
      "-19 155\n",
      "24 0\n",
      "28 0\n",
      "21 124\n",
      "11 124\n",
      "-15 133\n",
      "-41 142\n",
      "-32 142\n",
      "-40 133\n",
      "-42 129\n",
      "-30 142\n",
      "-24 150\n",
      "24 0\n",
      "27 0\n",
      "17 115\n",
      "6 111\n",
      "-18 120\n",
      "-47 129\n",
      "-35 133\n",
      "-47 124\n",
      "-52 124\n",
      "-33 124\n",
      "-29 137\n",
      "11 0\n",
      "17 0\n",
      "9 120\n",
      "1 115\n",
      "-23 124\n",
      "-50 120\n",
      "-63 120\n",
      "-62 124\n",
      "-52 129\n",
      "-29 124\n",
      "-27 133\n",
      "12 0\n",
      "16 0\n",
      "6 129\n",
      "-5 120\n",
      "-17 124\n",
      "-30 129\n",
      "-36 115\n",
      "-42 120\n",
      "-30 124\n",
      "-24 133\n",
      "-21 124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-363.)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 0\n",
    "for i in range(11):\n",
    "    for j in range(11):\n",
    "        t += mSample(16,8)(st['features.0.weight'][0,0,i,j] * images[0,0,i,j])\n",
    "        t = mSample(16,8)(t)\n",
    "        print int(st['features.0.weight'][0,0,i,j]*256) , int(images[0,0,i,j]*256)\n",
    "t*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 (py2env)",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
